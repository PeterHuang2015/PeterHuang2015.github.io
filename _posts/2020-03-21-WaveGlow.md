---
category: 'research'
title: 'WaveGlow'
note: 'fully convolutional neural VoCoder'
---
### Background

Since [WaveNet](https://deepmind.com/blog/article/wavenet-generative-model-raw-audio){:target="_blank"} got proposed in 2016, we are much closer to human-level speech generation. For SOTA End2End text-to-speech(TTS) systems, the whole pipeline commonly got splitted into two parts: text2feature and feature2wav, melspectrum often serve as feature for connecting two parts.

For feature2wave part(or called Voice Encoder, in short VoCoder), even WaveNet has been avaiable on Google Cloud for a long time, the topic on End2End TTS is still interesting for further exploration based on these observations:

+ It's high-dimension(or 1D but with millions of sampling points) sampling problem. High quality audio requries extremely high sampling rate(20KHz+) which raise the bar for generative model to work(like toy GAN generating 100X100 images vs GANs that works for generating 4K Images).
+ Speed is still a big concern when trying to deploy SOTA work online, original WaveNet is pure auto-regressive model and takes hours to generate few seconds audio wav. Things are getting better but still introduce considerable cost and engineering burden compared to classical parametric and concatentive approaches.
+ Interestingly, GAN does not dominate the field of VoCoders, instead, we see the raise of flow-based generative models by referencing to recent work.

In this post, we will focus on WaveGlow, introduce the work, provide a TensorRT implementation and looking for ways to optimize it's training phase, which may take months if you try to train from scratch.

### Fast Neural VoCoder for Text-to-Speech

[WaveGlow](https://github.com/NVIDIA/waveglow), as proposed by Nvidia ADLR team in late 2018 and published on ICASSP 2019, is a parallelization-friendly, fast VoCoder. From the algorithm, WaveNet+Glow.

Stepback, the rnn, conv, attention story.