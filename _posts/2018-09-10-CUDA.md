---
category: 'work'
title: 'CUDA 101'
note: 'to tame thousands of threads for super computing'
---

### Helper
+ The paper-Ian Buck's [thesis](http://graphics.stanford.edu/~ianbuck/thesis.pdf)
+ The book-[CUDA by example](https://developer.nvidia.com/cuda-example) and official [documents](https://docs.nvidia.com/cuda/)
+ The course- [CS344](https://classroom.udacity.com/courses/cs344/) on Udacity and corresponding Github [repo](https://github.com/udacity/cs344/)

### From Graphics to General Computing
This is the 1st post of a series of CUDA notes for helping engineers to tame thousands of threads on specific devices for their work. Written in English so more experts could help correcting the potential mistakes :)

The Mythbuster GPU vs CPU video might be the best one to show why GPU is faster:

<iframe src="http://v3-default.ixigua.com/4dcfd45941f7050177f9dfcb949965ea/5d8dcac1/video/m/220529c116c769e4b668bf6bd0e6b44fbd81163918db000078581872ca01/?a=2011&br=349&cr=0&cs=0&dr=0&ds=1&er=&l=2019092715375601001404701200132046&lr=&rc=am95ZjM0cXV0cDMzOzczM0ApNTk8ZzlnZWQ8Nzw1NjQ2N2dzMGgvMy1kajNfLS02LTBzcy8tY18tMTVhMC1eYy40YjA6Yw%3D%3D"> </iframe>

Back to 2006, guys in Stanford's Graphics lab tried to introduce the compute power of GPU to general computing and ship one product, the work got partially noted in Ian Buck's Phd thesis. Later the work influenced the hardware design and brought out CUDA(Compute Unified Device Architecture). From software prospective, it's extended from standard C programming language, exposed GPU's power to software engineers without requiring expertise in Graphics. Use nvcc as the compiler to split the work on CPU to gcc/vs etc. and turn GPU functions(kernels) to executables for CUDA Runtime(curt) which communicate with device drivers to schedule and perform compute jobs.

### More Threads->Need for Speed
The idea might be simple, use more threads to enpower executing the application in parallel. The more threads you have, the more time you save. And its easy to start writing such program:

```c
// from CS344
#include <stdio.h> 
#define NUM_BLOCKS 1
#define BLOCK_WIDTH 256
__global__ void hello()
{
 printf("Hello world! I'm thread %d\n", threadIdx.x);
}
int main(int argc,char **argv)
{
 // launch the kernel
 hello<<<NUM_BLOCKS, BLOCK_WIDTH>>>();
 // force the printf()s to flush
 cudaDeviceSynchronize();
 printf("That's all!\n");
 return 0;
}
```

### No Free Lunch
On the other hand, as u may know, when trying to solve a problem using multi-threads, you are having two problems now :) Despite figuring out how to turn the algorithm to be friendly for parallelization. Memory hierarchy also became more complex for serving such devices, like cudaMalloc's speed compared to its counterparts. Single thread is relatively weak on GPU and taming thousands of such threads are challenging but also rewarding, for getting a deeper understanding of computer architecture, deploy lower cost, higher throughput services.

### Take a Sip
If you are interested and in need of CUDA programming skill, pls start from reading official docs: mainly programming-guide and best-practice guide and do some quick labs for exploration. Some links got provided as below to help u kickstarting.

```python
import webbrower # pls execute the script in python
links = ['graphics.stanford.edu/~ianbuck/thesis.pdf', \ 
'https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html', \
'https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html', \
'https://developer.nvidia.com/cuda-example', \
'https://classroom.udacity.com/courses/cs344', \
'https://github.com/udacity/cs344/']
```